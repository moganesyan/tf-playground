{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c65998-af26-4391-b413-511f7c436525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08134b89-a873-422f-8d61-6ea7e05eb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b6d38b-7a48-43cd-928d-456166197693",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"..\")\n",
    "sys.path.append(str(BASE_DIR.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fcbf26-f1bd-44ae-b9ed-a736ed9ee16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3626e6-8437-49b8-8431-e0bda8c8bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline import get_transform_func\n",
    "from losses import ntxent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34f6bd-5761-4575-9727-3b9af6fc6843",
   "metadata": {},
   "source": [
    "# Create Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e048e6b2-a2ee-41d4-9d94-8cc01a1b01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ad5f91-86ed-48a8-a774-8086fc8df30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = x_train.shape[1:]\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961f0b3f-5279-4f66-9522-4893caa07bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 17:14:50.809478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:50.815570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:50.816055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:50.817302: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-27 17:14:50.817699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:50.818166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:50.818598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:51.240447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:51.240900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:51.241371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 17:14:51.241779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7022 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6acc82f4-f8e3-4717-adf8-2f00b18000a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_func = get_transform_func(\n",
    "    NUM_CLASSES, blur = False, distort_strength = 0.50,\n",
    "    crop_size = (0.30, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0003b1-24a9-4ae9-94ff-327f8e7533f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset\n",
    "train_data = train_data.map(\n",
    "    transform_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_data = train_data.cache()\n",
    "train_data = train_data.shuffle(buffer_size=1024)\n",
    "train_data = train_data.batch(BATCH_SIZE, drop_remainder = True)\n",
    "train_data = train_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9511bb-7ee4-4ef7-bbbc-daaf7e6bf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "test_data = test_data.map(\n",
    "    transform_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_data = test_data.cache()\n",
    "test_data = test_data.batch(BATCH_SIZE, drop_remainder = True)\n",
    "test_data = test_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb78975-66ae-4641-9a64-8dc2e6054d1c",
   "metadata": {},
   "source": [
    "# Create SimClr Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7619837f-4831-49d2-9c91-69cc2672a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetSimClr(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 name = None):\n",
    "        super(AlexNetSimClr, self).__init__(name = name)\n",
    "        self._input_shape = input_shape\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        # encoder / backbone\n",
    "        self._conv1 = tf.keras.layers.Conv2D(\n",
    "            96, 11, 2, 'same', name = 'bbone_conv1')\n",
    "        self._conv2 = tf.keras.layers.Conv2D(\n",
    "            256, 5, 1, 'same', name = 'bbone_conv2')\n",
    "        self._conv3 = tf.keras.layers.Conv2D(\n",
    "            384, 3, 2, 'same', name = 'bbone_conv3')\n",
    "        self._conv4 = tf.keras.layers.Conv2D(\n",
    "            384, 3, 1, 'same', name = 'bbone_conv4')\n",
    "        self._conv5 = tf.keras.layers.Conv2D(\n",
    "            356, 3, 1, 'same', name = 'bbone_conv5')        \n",
    "        self._gpool2d = tf.keras.layers.GlobalAveragePooling2D(name = 'bbone_gpool')\n",
    "        \n",
    "        # projection vectors\n",
    "        self._fc1 = tf.keras.layers.Dense(128, name = 'proj_fc1')\n",
    "        self._fc2 = tf.keras.layers.Dense(128, name = 'proj_fc2')\n",
    "\n",
    "    def call(self, x_in):\n",
    "        # run encoder\n",
    "        x = self._conv1(x_in)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu1')\n",
    "        x = self._conv2(x)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu2')\n",
    "        x = self._conv3(x)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu3')\n",
    "        x = self._conv4(x)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu4')\n",
    "        x = self._conv5(x)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu5')\n",
    "        x = self._gpool2d(x)\n",
    "        \n",
    "        # run projection\n",
    "        x = self._fc1(x)\n",
    "        x = tf.nn.relu(x, name = 'relu6')\n",
    "        x_out = self._fc2(x)\n",
    "\n",
    "        return x_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f43c9116-6079-4d76-9c48-cda4e3c86a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr = AlexNetSimClr(INPUT_SHAPE, NUM_CLASSES, 'alexnet_simclr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bfbcc88-0212-4ac7-9518-d83c50916364",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "LEARNING_RATE = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ec759b-2894-4512-b8c5-df6452b11101",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=tf.keras.optimizers.Adam(learning_rate = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a9a28-2ff5-4532-a4b7-ad86e75bc10f",
   "metadata": {},
   "source": [
    "# Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d97149bd-2cfb-4260-9c11-7445c645ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x_batch_a, x_batch_b):\n",
    "    with tf.GradientTape() as tape:\n",
    "        proj_a = simclr(x_batch_a)\n",
    "        proj_b = simclr(x_batch_b)\n",
    "        loss_value = ntxent_loss(proj_a, proj_b)\n",
    "    \n",
    "    opt.minimize(loss_value, simclr.trainable_variables, tape = tape)\n",
    "\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405de33-3359-4f20-a1fc-499ff621803b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f46bf215-39bf-45c1-b74f-f918b37fcb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epoch: 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 17:14:54.446815: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is 7.90.\n",
      "Loss at step 350 is 4.33.\n",
      "Working on epoch: 1...\n",
      "Loss at step 0 is 4.30.\n",
      "Loss at step 350 is 4.24.\n",
      "Working on epoch: 2...\n",
      "Loss at step 0 is 4.26.\n",
      "Loss at step 350 is 4.22.\n",
      "Working on epoch: 3...\n",
      "Loss at step 0 is 4.19.\n",
      "Loss at step 350 is 4.22.\n",
      "Working on epoch: 4...\n",
      "Loss at step 0 is 4.20.\n",
      "Loss at step 350 is 4.24.\n",
      "Working on epoch: 5...\n",
      "Loss at step 0 is 4.16.\n",
      "Loss at step 350 is 4.16.\n",
      "Working on epoch: 6...\n",
      "Loss at step 0 is 4.22.\n",
      "Loss at step 350 is 4.16.\n",
      "Working on epoch: 7...\n",
      "Loss at step 0 is 4.13.\n",
      "Loss at step 350 is 4.17.\n",
      "Working on epoch: 8...\n",
      "Loss at step 0 is 4.12.\n",
      "Loss at step 350 is 4.14.\n",
      "Working on epoch: 9...\n",
      "Loss at step 0 is 4.12.\n",
      "Loss at step 350 is 4.14.\n",
      "Working on epoch: 10...\n",
      "Loss at step 0 is 4.13.\n",
      "Loss at step 350 is 4.13.\n",
      "Working on epoch: 11...\n",
      "Loss at step 0 is 4.12.\n",
      "Loss at step 350 is 4.15.\n",
      "Working on epoch: 12...\n",
      "Loss at step 0 is 4.11.\n",
      "Loss at step 350 is 4.15.\n",
      "Working on epoch: 13...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 4.11.\n",
      "Working on epoch: 14...\n",
      "Loss at step 0 is 4.12.\n",
      "Loss at step 350 is 4.11.\n",
      "Working on epoch: 15...\n",
      "Loss at step 0 is 4.09.\n",
      "Loss at step 350 is 4.10.\n",
      "Working on epoch: 16...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 4.10.\n",
      "Working on epoch: 17...\n",
      "Loss at step 0 is 4.12.\n",
      "Loss at step 350 is 4.13.\n",
      "Working on epoch: 18...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 4.09.\n",
      "Working on epoch: 19...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 4.10.\n",
      "Working on epoch: 20...\n",
      "Loss at step 0 is 4.06.\n",
      "Loss at step 350 is 4.12.\n",
      "Working on epoch: 21...\n",
      "Loss at step 0 is 4.07.\n",
      "Loss at step 350 is 4.09.\n",
      "Working on epoch: 22...\n",
      "Loss at step 0 is 4.07.\n",
      "Loss at step 350 is 4.08.\n",
      "Working on epoch: 23...\n",
      "Loss at step 0 is 4.09.\n",
      "Loss at step 350 is 4.07.\n",
      "Working on epoch: 24...\n",
      "Loss at step 0 is 4.05.\n",
      "Loss at step 350 is 4.08.\n",
      "Working on epoch: 25...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 4.08.\n",
      "Working on epoch: 26...\n",
      "Loss at step 0 is 4.07.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 27...\n",
      "Loss at step 0 is 4.06.\n",
      "Loss at step 350 is 4.04.\n",
      "Working on epoch: 28...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 29...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 4.04.\n",
      "Working on epoch: 30...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 31...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 4.10.\n",
      "Working on epoch: 32...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 4.02.\n",
      "Working on epoch: 33...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 34...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 35...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 4.02.\n",
      "Working on epoch: 36...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 37...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 38...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 39...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 4.00.\n",
      "Working on epoch: 40...\n",
      "Loss at step 0 is 4.00.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 41...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 42...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 4.00.\n",
      "Working on epoch: 43...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 4.00.\n",
      "Working on epoch: 44...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 45...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 46...\n",
      "Loss at step 0 is 4.00.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 47...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 48...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 49...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 50...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 51...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 52...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 53...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 54...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 55...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 56...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 57...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 58...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 59...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 60...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 61...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 62...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 63...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 64...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 65...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 66...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 67...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 68...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 69...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 70...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 71...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 72...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 73...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 74...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 75...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 76...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 77...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 78...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 79...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 80...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 81...\n",
      "Loss at step 0 is 3.92.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 82...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 83...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 84...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 85...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 86...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 87...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 88...\n",
      "Loss at step 0 is 3.92.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 89...\n",
      "Loss at step 0 is 4.06.\n",
      "Loss at step 350 is 4.08.\n",
      "Working on epoch: 90...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 91...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 92...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 93...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 94...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 95...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.88.\n",
      "Working on epoch: 96...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 97...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 98...\n",
      "Loss at step 0 is 4.39.\n",
      "Loss at step 350 is 4.04.\n",
      "Working on epoch: 99...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 100...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 101...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 102...\n",
      "Loss at step 0 is 4.05.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 103...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 104...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 105...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.88.\n",
      "Working on epoch: 106...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 107...\n",
      "Loss at step 0 is 3.92.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 108...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 109...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 110...\n",
      "Loss at step 0 is 4.00.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 111...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 112...\n",
      "Loss at step 0 is 3.87.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 113...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 114...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 115...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 116...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 4.06.\n",
      "Working on epoch: 117...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 118...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 119...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 120...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 121...\n",
      "Loss at step 0 is 4.31.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 122...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 123...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 124...\n",
      "Loss at step 0 is 3.92.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 125...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.88.\n",
      "Working on epoch: 126...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 127...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 128...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 129...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 130...\n",
      "Loss at step 0 is 3.87.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 131...\n",
      "Loss at step 0 is 4.09.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 132...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 133...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 134...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 4.07.\n",
      "Working on epoch: 135...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 136...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 137...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 138...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 139...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 140...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 141...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 142...\n",
      "Loss at step 0 is 4.21.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 143...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 144...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 145...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 146...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 147...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 148...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 149...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 150...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 151...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 152...\n",
      "Loss at step 0 is 4.29.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 153...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 154...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 155...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 156...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 157...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 158...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 4.15.\n",
      "Working on epoch: 159...\n",
      "Loss at step 0 is 3.91.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 160...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 161...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 162...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 163...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 164...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 4.14.\n",
      "Working on epoch: 165...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 166...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 4.09.\n",
      "Working on epoch: 167...\n",
      "Loss at step 0 is 4.11.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 168...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 169...\n",
      "Loss at step 0 is 3.81.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 170...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 171...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 172...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 173...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 4.09.\n",
      "Working on epoch: 174...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 175...\n",
      "Loss at step 0 is 4.09.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 176...\n",
      "Loss at step 0 is 4.19.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 177...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 178...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 179...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 180...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 4.07.\n",
      "Working on epoch: 181...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 182...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 183...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 184...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 185...\n",
      "Loss at step 0 is 4.31.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 186...\n",
      "Loss at step 0 is 3.81.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 187...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 188...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 189...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 190...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 4.10.\n",
      "Working on epoch: 191...\n",
      "Loss at step 0 is 4.05.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 192...\n",
      "Loss at step 0 is 3.81.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 193...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 4.00.\n",
      "Working on epoch: 194...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 195...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 196...\n",
      "Loss at step 0 is 4.50.\n",
      "Loss at step 350 is 4.11.\n",
      "Working on epoch: 197...\n",
      "Loss at step 0 is 4.09.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 198...\n",
      "Loss at step 0 is 3.87.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 199...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.87.\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "        Train EfficientNet SimClr on CIFAR10.\n",
    "    \"\"\"\n",
    "\n",
    "    losses_train = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Working on epoch: {epoch}...\")\n",
    "        for train_idx, (x_batch_a, x_batch_b, y_batch_train) in enumerate(train_data):\n",
    "            loss_value_train = train_step(x_batch_a, x_batch_b)\n",
    "            losses_train.append(loss_value_train.numpy())\n",
    "\n",
    "            if train_idx % 350 == 0:\n",
    "                print(f\"Loss at step {train_idx} is {loss_value_train.numpy():.2f}.\")\n",
    "    return losses_train\n",
    "\n",
    "\n",
    "# train and plot loss\n",
    "train_losses = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4cc07-1c04-4f29-bf27-fa41d423e05c",
   "metadata": {},
   "source": [
    "# Visualise Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d6d9dba-9a60-4102-b5b9-67c2dffe5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a5b011a-e417-4699-bec4-c1819e7c3cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9539975a90>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25bda9-42ef-447b-aa95-4bc2d9c4b467",
   "metadata": {},
   "source": [
    "# Save Weights & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0038480b-1200-47f0-a346-4fd2818ee34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_dir = Path(\"assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c7f479-88e5-4f75-b35d-8b005de42ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = asset_dir / 'pretrain/weights'\n",
    "if not weights_path.exists():\n",
    "    weights_path.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fae64d3-c373-4354-98ef-72e0f9ce60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr.save_weights(weights_path / 'model_encoder_500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2898dc1c-55ed-4d7f-aaf1-b591773e424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = asset_dir / 'pretrain/model'\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa7347f-2af1-4f5f-b1af-9946f9ace5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 20:50:34.536536: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets/pretrain/model/model_encoder_500.tf/assets\n"
     ]
    }
   ],
   "source": [
    "simclr.save(model_path / 'model_encoder_500.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb4e52-15a6-414f-9a11-dc3b5b7f0ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
