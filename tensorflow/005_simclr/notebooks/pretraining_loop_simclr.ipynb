{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c65998-af26-4391-b413-511f7c436525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08134b89-a873-422f-8d61-6ea7e05eb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b6d38b-7a48-43cd-928d-456166197693",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"..\")\n",
    "sys.path.append(str(BASE_DIR.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fcbf26-f1bd-44ae-b9ed-a736ed9ee16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3626e6-8437-49b8-8431-e0bda8c8bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline import get_transform_func\n",
    "from losses import ntxent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34f6bd-5761-4575-9727-3b9af6fc6843",
   "metadata": {},
   "source": [
    "# Create Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e048e6b2-a2ee-41d4-9d94-8cc01a1b01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ad5f91-86ed-48a8-a774-8086fc8df30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = x_train.shape[1:]\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961f0b3f-5279-4f66-9522-4893caa07bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 00:27:25.792088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:25.799461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:25.800051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:25.801362: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 00:27:25.801827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:25.802360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:25.802845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:26.294147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:26.294786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:26.295339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-04 00:27:26.295886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7008 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6acc82f4-f8e3-4717-adf8-2f00b18000a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_func = get_transform_func(\n",
    "    NUM_CLASSES, blur = False, crop_resize = True, distort_strength = 0.50,\n",
    "    crop_size = (0.30, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0003b1-24a9-4ae9-94ff-327f8e7533f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset\n",
    "train_data = train_data.map(\n",
    "    transform_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_data = train_data.cache()\n",
    "train_data = train_data.shuffle(buffer_size=1024)\n",
    "train_data = train_data.batch(BATCH_SIZE, drop_remainder = True)\n",
    "train_data = train_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9511bb-7ee4-4ef7-bbbc-daaf7e6bf2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "test_data = test_data.map(\n",
    "    transform_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_data = test_data.cache()\n",
    "test_data = test_data.batch(BATCH_SIZE, drop_remainder = True)\n",
    "test_data = test_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb78975-66ae-4641-9a64-8dc2e6054d1c",
   "metadata": {},
   "source": [
    "# Create SimClr Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7619837f-4831-49d2-9c91-69cc2672a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetSimClr(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 name = None):\n",
    "        super(AlexNetSimClr, self).__init__(name = name)\n",
    "        self._input_shape = input_shape\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        # encoder / backbone\n",
    "        self._conv1 = tf.keras.layers.Conv2D(\n",
    "            96, 11, 2, 'same', name = 'bbone_conv1')\n",
    "        self._conv2 = tf.keras.layers.Conv2D(\n",
    "            256, 5, 1, 'same', name = 'bbone_conv2')\n",
    "        self._conv3 = tf.keras.layers.Conv2D(\n",
    "            384, 3, 2, 'same', name = 'bbone_conv3')\n",
    "        self._conv4 = tf.keras.layers.Conv2D(\n",
    "            384, 3, 1, 'same', name = 'bbone_conv4')\n",
    "        self._conv5 = tf.keras.layers.Conv2D(\n",
    "            356, 3, 1, 'same', name = 'bbone_conv5')        \n",
    "        self._gpool2d = tf.keras.layers.GlobalAveragePooling2D(name = 'bbone_gpool')\n",
    "        \n",
    "        # projection vectors\n",
    "        self._fc1 = tf.keras.layers.Dense(128, name = 'proj_fc1')\n",
    "        self._fc2 = tf.keras.layers.Dense(128, name = 'proj_fc2')\n",
    "\n",
    "    def call(self, x_in):\n",
    "        # run encoder\n",
    "        x = self._conv1(x_in)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu1')\n",
    "        x = self._conv2(x)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu2')\n",
    "        x = self._conv3(x)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu3')\n",
    "        x = self._conv4(x)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu4')\n",
    "        x = self._conv5(x)\n",
    "        x = tf.nn.relu(x, name = 'bbone_relu5')\n",
    "        x = self._gpool2d(x)\n",
    "        \n",
    "        # run projection\n",
    "        x = self._fc1(x)\n",
    "        x = tf.nn.relu(x, name = 'relu6')\n",
    "        x_out = self._fc2(x)\n",
    "\n",
    "        return x_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f43c9116-6079-4d76-9c48-cda4e3c86a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr = AlexNetSimClr(INPUT_SHAPE, NUM_CLASSES, 'alexnet_simclr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bfbcc88-0212-4ac7-9518-d83c50916364",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "DECAY_STEPS = EPOCHS * (len(x_train) // BATCH_SIZE)\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c1eab1-4f2f-453b-9576-8242f6188c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decayed_learning_rate(step, alpha = 0.0001):\n",
    "    step = min(step, DECAY_STEPS)\n",
    "    cosine_decay = 0.5 * (1 + np.cos(np.pi * step / DECAY_STEPS))\n",
    "    decayed = (1 - alpha) * cosine_decay + alpha\n",
    "    return LEARNING_RATE * decayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b1cbef-a627-4235-8a04-6b6471db4d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DECAY_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e67e8b6e-76da-4854-9daf-632ab4fa03ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2530180790>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([decayed_learning_rate(i) for i in np.arange(DECAY_STEPS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0ec759b-2894-4512-b8c5-df6452b11101",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    LEARNING_RATE, DECAY_STEPS, alpha = 0.0001)\n",
    "opt=tf.keras.optimizers.Adam(learning_rate = lr_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a9a28-2ff5-4532-a4b7-ad86e75bc10f",
   "metadata": {},
   "source": [
    "# Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d97149bd-2cfb-4260-9c11-7445c645ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x_batch_a, x_batch_b):\n",
    "    with tf.GradientTape() as tape:\n",
    "        proj_a = simclr(x_batch_a)\n",
    "        proj_b = simclr(x_batch_b)\n",
    "        loss_value = ntxent_loss(proj_a, proj_b)\n",
    "    \n",
    "    opt.minimize(loss_value, simclr.trainable_variables, tape = tape)\n",
    "\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405de33-3359-4f20-a1fc-499ff621803b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f46bf215-39bf-45c1-b74f-f918b37fcb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epoch: 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 00:27:31.173321: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is 4.95.\n",
      "Loss at step 350 is 4.34.\n",
      "Working on epoch: 1...\n",
      "Loss at step 0 is 4.34.\n",
      "Loss at step 350 is 4.27.\n",
      "Working on epoch: 2...\n",
      "Loss at step 0 is 4.29.\n",
      "Loss at step 350 is 4.26.\n",
      "Working on epoch: 3...\n",
      "Loss at step 0 is 4.17.\n",
      "Loss at step 350 is 4.24.\n",
      "Working on epoch: 4...\n",
      "Loss at step 0 is 4.24.\n",
      "Loss at step 350 is 4.19.\n",
      "Working on epoch: 5...\n",
      "Loss at step 0 is 4.20.\n",
      "Loss at step 350 is 4.20.\n",
      "Working on epoch: 6...\n",
      "Loss at step 0 is 4.18.\n",
      "Loss at step 350 is 4.17.\n",
      "Working on epoch: 7...\n",
      "Loss at step 0 is 4.17.\n",
      "Loss at step 350 is 4.19.\n",
      "Working on epoch: 8...\n",
      "Loss at step 0 is 4.13.\n",
      "Loss at step 350 is 4.14.\n",
      "Working on epoch: 9...\n",
      "Loss at step 0 is 4.17.\n",
      "Loss at step 350 is 4.16.\n",
      "Working on epoch: 10...\n",
      "Loss at step 0 is 4.15.\n",
      "Loss at step 350 is 4.14.\n",
      "Working on epoch: 11...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 4.17.\n",
      "Working on epoch: 12...\n",
      "Loss at step 0 is 4.13.\n",
      "Loss at step 350 is 4.14.\n",
      "Working on epoch: 13...\n",
      "Loss at step 0 is 4.12.\n",
      "Loss at step 350 is 4.12.\n",
      "Working on epoch: 14...\n",
      "Loss at step 0 is 4.14.\n",
      "Loss at step 350 is 4.12.\n",
      "Working on epoch: 15...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 4.12.\n",
      "Working on epoch: 16...\n",
      "Loss at step 0 is 4.12.\n",
      "Loss at step 350 is 4.12.\n",
      "Working on epoch: 17...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 4.10.\n",
      "Working on epoch: 18...\n",
      "Loss at step 0 is 4.08.\n",
      "Loss at step 350 is 4.09.\n",
      "Working on epoch: 19...\n",
      "Loss at step 0 is 4.08.\n",
      "Loss at step 350 is 4.08.\n",
      "Working on epoch: 20...\n",
      "Loss at step 0 is 4.06.\n",
      "Loss at step 350 is 4.09.\n",
      "Working on epoch: 21...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 4.07.\n",
      "Working on epoch: 22...\n",
      "Loss at step 0 is 4.09.\n",
      "Loss at step 350 is 4.07.\n",
      "Working on epoch: 23...\n",
      "Loss at step 0 is 4.07.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 24...\n",
      "Loss at step 0 is 4.05.\n",
      "Loss at step 350 is 4.04.\n",
      "Working on epoch: 25...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 4.07.\n",
      "Working on epoch: 26...\n",
      "Loss at step 0 is 4.07.\n",
      "Loss at step 350 is 4.04.\n",
      "Working on epoch: 27...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 28...\n",
      "Loss at step 0 is 4.00.\n",
      "Loss at step 350 is 4.04.\n",
      "Working on epoch: 29...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 30...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 31...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 4.02.\n",
      "Working on epoch: 32...\n",
      "Loss at step 0 is 4.00.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 33...\n",
      "Loss at step 0 is 4.00.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 34...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 4.00.\n",
      "Working on epoch: 35...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 36...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 37...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 38...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 39...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 40...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 41...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 42...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 43...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 44...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 45...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 46...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 47...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 48...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 49...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 50...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.95.\n",
      "Working on epoch: 51...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 52...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 53...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 54...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 55...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 56...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 57...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 58...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 59...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 60...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 61...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 62...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 63...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 64...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 65...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 66...\n",
      "Loss at step 0 is 4.18.\n",
      "Loss at step 350 is 4.09.\n",
      "Working on epoch: 67...\n",
      "Loss at step 0 is 3.92.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 68...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 69...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 70...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 71...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 72...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 73...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 74...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 75...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 76...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 77...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 78...\n",
      "Loss at step 0 is 3.92.\n",
      "Loss at step 350 is 4.08.\n",
      "Working on epoch: 79...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 80...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 4.10.\n",
      "Working on epoch: 81...\n",
      "Loss at step 0 is 4.07.\n",
      "Loss at step 350 is 4.18.\n",
      "Working on epoch: 82...\n",
      "Loss at step 0 is 4.08.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 83...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 84...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 85...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 86...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 87...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 88...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 4.12.\n",
      "Working on epoch: 89...\n",
      "Loss at step 0 is 4.08.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 90...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 91...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 92...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 93...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 94...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 95...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 96...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 97...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 98...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 99...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 100...\n",
      "Loss at step 0 is 3.87.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 101...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 102...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 103...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 104...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 105...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 106...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 107...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 108...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 109...\n",
      "Loss at step 0 is 3.96.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 110...\n",
      "Loss at step 0 is 3.92.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 111...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 112...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 113...\n",
      "Loss at step 0 is 3.87.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 114...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 115...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 116...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 117...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 118...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 4.68.\n",
      "Working on epoch: 119...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 120...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 121...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 122...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.88.\n",
      "Working on epoch: 123...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 124...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 125...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 126...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 127...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 4.13.\n",
      "Working on epoch: 128...\n",
      "Loss at step 0 is 4.08.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 129...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 4.02.\n",
      "Working on epoch: 130...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 131...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 132...\n",
      "Loss at step 0 is 3.87.\n",
      "Loss at step 350 is 4.00.\n",
      "Working on epoch: 133...\n",
      "Loss at step 0 is 4.00.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 134...\n",
      "Loss at step 0 is 3.94.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 135...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.88.\n",
      "Working on epoch: 136...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 4.06.\n",
      "Working on epoch: 137...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 138...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 139...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 140...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 141...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 142...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 143...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 144...\n",
      "Loss at step 0 is 4.00.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 145...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 146...\n",
      "Loss at step 0 is 3.87.\n",
      "Loss at step 350 is 4.10.\n",
      "Working on epoch: 147...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 148...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 149...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 150...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 151...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 152...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 153...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 154...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 155...\n",
      "Loss at step 0 is 3.81.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 156...\n",
      "Loss at step 0 is 3.81.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 157...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 4.43.\n",
      "Working on epoch: 158...\n",
      "Loss at step 0 is 4.09.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 159...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 160...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 4.20.\n",
      "Working on epoch: 161...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 162...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 163...\n",
      "Loss at step 0 is 4.03.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 164...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 165...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 166...\n",
      "Loss at step 0 is 3.81.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 167...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 168...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 169...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 170...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 171...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 172...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 173...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 174...\n",
      "Loss at step 0 is 3.92.\n",
      "Loss at step 350 is 3.89.\n",
      "Working on epoch: 175...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 176...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 177...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 178...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 179...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 4.00.\n",
      "Working on epoch: 180...\n",
      "Loss at step 0 is 3.86.\n",
      "Loss at step 350 is 4.08.\n",
      "Working on epoch: 181...\n",
      "Loss at step 0 is 3.88.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 182...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 183...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 184...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 185...\n",
      "Loss at step 0 is 3.87.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 186...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 187...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 188...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 189...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 190...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 191...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 192...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 193...\n",
      "Loss at step 0 is 4.13.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 194...\n",
      "Loss at step 0 is 4.01.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 195...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 196...\n",
      "Loss at step 0 is 3.81.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 197...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 4.02.\n",
      "Working on epoch: 198...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 199...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 4.09.\n",
      "Working on epoch: 200...\n",
      "Loss at step 0 is 4.06.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 201...\n",
      "Loss at step 0 is 3.97.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 202...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.88.\n",
      "Working on epoch: 203...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 204...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 205...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 3.90.\n",
      "Working on epoch: 206...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.84.\n",
      "Working on epoch: 207...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 208...\n",
      "Loss at step 0 is 3.73.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 209...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 4.04.\n",
      "Working on epoch: 210...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 211...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 212...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 213...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 3.94.\n",
      "Working on epoch: 214...\n",
      "Loss at step 0 is 3.93.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 215...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 216...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.88.\n",
      "Working on epoch: 217...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 218...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 219...\n",
      "Loss at step 0 is 3.83.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 220...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 4.29.\n",
      "Working on epoch: 221...\n",
      "Loss at step 0 is 3.90.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 222...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 223...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 224...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 225...\n",
      "Loss at step 0 is 3.73.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 226...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 227...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 228...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 229...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 230...\n",
      "Loss at step 0 is 3.82.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 231...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 232...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 233...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 234...\n",
      "Loss at step 0 is 3.73.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 235...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 236...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.83.\n",
      "Working on epoch: 237...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 238...\n",
      "Loss at step 0 is 3.73.\n",
      "Loss at step 350 is 3.77.\n",
      "Working on epoch: 239...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.92.\n",
      "Working on epoch: 240...\n",
      "Loss at step 0 is 3.85.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 241...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 242...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 243...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 3.93.\n",
      "Working on epoch: 244...\n",
      "Loss at step 0 is 3.89.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 245...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 246...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.80.\n",
      "Working on epoch: 247...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 248...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 249...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 250...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 251...\n",
      "Loss at step 0 is 3.73.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 252...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 253...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 4.03.\n",
      "Working on epoch: 254...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 255...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 256...\n",
      "Loss at step 0 is 4.76.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 257...\n",
      "Loss at step 0 is 3.79.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 258...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 259...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 260...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 261...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 3.79.\n",
      "Working on epoch: 262...\n",
      "Loss at step 0 is 3.76.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 263...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 264...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 265...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 266...\n",
      "Loss at step 0 is 3.73.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 267...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 268...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 269...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 270...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 271...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 4.01.\n",
      "Working on epoch: 272...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.87.\n",
      "Working on epoch: 273...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 274...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 275...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 276...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 277...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 278...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 279...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 280...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 281...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 282...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 283...\n",
      "Loss at step 0 is 4.40.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 284...\n",
      "Loss at step 0 is 4.06.\n",
      "Loss at step 350 is 4.02.\n",
      "Working on epoch: 285...\n",
      "Loss at step 0 is 4.02.\n",
      "Loss at step 350 is 3.99.\n",
      "Working on epoch: 286...\n",
      "Loss at step 0 is 3.99.\n",
      "Loss at step 350 is 3.98.\n",
      "Working on epoch: 287...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.97.\n",
      "Working on epoch: 288...\n",
      "Loss at step 0 is 3.98.\n",
      "Loss at step 350 is 3.96.\n",
      "Working on epoch: 289...\n",
      "Loss at step 0 is 3.95.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 290...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 291...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 3.75.\n",
      "Working on epoch: 292...\n",
      "Loss at step 0 is 3.74.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 293...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 294...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 295...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 296...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 297...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 298...\n",
      "Loss at step 0 is 3.75.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 299...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 300...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 4.05.\n",
      "Working on epoch: 301...\n",
      "Loss at step 0 is 4.04.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 302...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 303...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 304...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 305...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 306...\n",
      "Loss at step 0 is 4.10.\n",
      "Loss at step 350 is 3.82.\n",
      "Working on epoch: 307...\n",
      "Loss at step 0 is 3.73.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 308...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.86.\n",
      "Working on epoch: 309...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 310...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 311...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 312...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.85.\n",
      "Working on epoch: 313...\n",
      "Loss at step 0 is 3.80.\n",
      "Loss at step 350 is 3.73.\n",
      "Working on epoch: 314...\n",
      "Loss at step 0 is 3.91.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 315...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 316...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 317...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 318...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.78.\n",
      "Working on epoch: 319...\n",
      "Loss at step 0 is 3.84.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 320...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 321...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 322...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 323...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 324...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 325...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.81.\n",
      "Working on epoch: 326...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 327...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 328...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 329...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 330...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 331...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 332...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 333...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 334...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 335...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 336...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 337...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 338...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.70.\n",
      "Working on epoch: 339...\n",
      "Loss at step 0 is 3.73.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 340...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 341...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 342...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.88.\n",
      "Working on epoch: 343...\n",
      "Loss at step 0 is 3.78.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 344...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 345...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.76.\n",
      "Working on epoch: 346...\n",
      "Loss at step 0 is 3.72.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 347...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 348...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.91.\n",
      "Working on epoch: 349...\n",
      "Loss at step 0 is 3.77.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 350...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 351...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 352...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.74.\n",
      "Working on epoch: 353...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 354...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 355...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 356...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 357...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 358...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 359...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 360...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 361...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 362...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 363...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 364...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 365...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 366...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 367...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 368...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 369...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.71.\n",
      "Working on epoch: 370...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 371...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 372...\n",
      "Loss at step 0 is 3.71.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 373...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 374...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 375...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 376...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 377...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 378...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 379...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 380...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 381...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 382...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 383...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 384...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 385...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 386...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 387...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 388...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 389...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 390...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 391...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 392...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 393...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 394...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.72.\n",
      "Working on epoch: 395...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 396...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 397...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 398...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 399...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 400...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 401...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 402...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 403...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 404...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 405...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 406...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 407...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 408...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 409...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 410...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 411...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 412...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 413...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 414...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 415...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 416...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 417...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 418...\n",
      "Loss at step 0 is 3.70.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 419...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.69.\n",
      "Working on epoch: 420...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 421...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 422...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 423...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 424...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 425...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 426...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 427...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 428...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 429...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 430...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 431...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.68.\n",
      "Working on epoch: 432...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 433...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 434...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 435...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 436...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 437...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 438...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 439...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 440...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 441...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 442...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 443...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 444...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 445...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 446...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 447...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 448...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 449...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 450...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 451...\n",
      "Loss at step 0 is 3.69.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 452...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 453...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 454...\n",
      "Loss at step 0 is 3.67.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 455...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 456...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 457...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 458...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.67.\n",
      "Working on epoch: 459...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 460...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 461...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 462...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 463...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 464...\n",
      "Loss at step 0 is 3.68.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 465...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 466...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 467...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 468...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 469...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 470...\n",
      "Loss at step 0 is 3.62.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 471...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 472...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 473...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 474...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 475...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 476...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 477...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 478...\n",
      "Loss at step 0 is 3.62.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 479...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 480...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 481...\n",
      "Loss at step 0 is 3.62.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 482...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 483...\n",
      "Loss at step 0 is 3.62.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 484...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 485...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 486...\n",
      "Loss at step 0 is 3.64.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 487...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.62.\n",
      "Working on epoch: 488...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 489...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 490...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 491...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 492...\n",
      "Loss at step 0 is 3.66.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 493...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 494...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.64.\n",
      "Working on epoch: 495...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 496...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.66.\n",
      "Working on epoch: 497...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.63.\n",
      "Working on epoch: 498...\n",
      "Loss at step 0 is 3.63.\n",
      "Loss at step 350 is 3.65.\n",
      "Working on epoch: 499...\n",
      "Loss at step 0 is 3.65.\n",
      "Loss at step 350 is 3.66.\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "        Train EfficientNet SimClr on CIFAR10.\n",
    "    \"\"\"\n",
    "\n",
    "    losses_train = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Working on epoch: {epoch}...\")\n",
    "        for train_idx, (x_batch_a, x_batch_b, y_batch_train) in enumerate(train_data):\n",
    "            loss_value_train = train_step(x_batch_a, x_batch_b)\n",
    "            losses_train.append(loss_value_train.numpy())\n",
    "\n",
    "            if train_idx % 350 == 0:\n",
    "                print(f\"Loss at step {train_idx} is {loss_value_train.numpy():.2f}.\")\n",
    "    return losses_train\n",
    "\n",
    "\n",
    "# train and plot loss\n",
    "train_losses = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4cc07-1c04-4f29-bf27-fa41d423e05c",
   "metadata": {},
   "source": [
    "# Visualise Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d6d9dba-9a60-4102-b5b9-67c2dffe5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a5b011a-e417-4699-bec4-c1819e7c3cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f25104314f0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25bda9-42ef-447b-aa95-4bc2d9c4b467",
   "metadata": {},
   "source": [
    "# Save Weights & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0038480b-1200-47f0-a346-4fd2818ee34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_dir = Path(\"assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1c7f479-88e5-4f75-b35d-8b005de42ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = asset_dir / 'pretrain/weights'\n",
    "if not weights_path.exists():\n",
    "    weights_path.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fae64d3-c373-4354-98ef-72e0f9ce60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr.save_weights(weights_path / f'model_encoder_{EPOCHS}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2898dc1c-55ed-4d7f-aaf1-b591773e424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = asset_dir / 'pretrain/model'\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2aa7347f-2af1-4f5f-b1af-9946f9ace5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 07:34:35.162302: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets/pretrain/model/model_encoder_200.tf/assets\n"
     ]
    }
   ],
   "source": [
    "simclr.save(model_path / f'model_encoder_{EPOCHS}.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bdf7fa-e163-4571-8fdc-40870a598f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
