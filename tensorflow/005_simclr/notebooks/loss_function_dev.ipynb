{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1bde282-7c9a-445f-a3ea-fb452796f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0679480d-c051-46f4-a703-87f1993439a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226de314-880f-4395-a577-9243bb69110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"..\")\n",
    "sys.path.append(str(BASE_DIR.resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85d8d8b-097c-4190-a42e-efa331fbe3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses_dev import ntxent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8f3a5c-ac2d-435b-9c1a-11e5c40b459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43577234-f220-4ed0-9f6d-d2f26d6a9c38",
   "metadata": {},
   "source": [
    "# Define Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c3a564b-da98-4d69-ac66-c822d007e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_similarity(u: tf.Tensor, v: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "        Calculate pairwise similarity between two vectors\n",
    "            using cos distance.\n",
    "\n",
    "        args:\n",
    "            u: tf.Tensor - First input vector.\n",
    "            v: tf.Tensor - Second input vector.\n",
    "        returns:\n",
    "            score: tf.Tensor - Similarity score scalar.\n",
    "    \"\"\"\n",
    "    \n",
    "    numer = tf.tensordot(u, v, axes = [[1],[1]])\n",
    "    denom = tf.norm(u, 2, axis = 1) * tf.norm(v, 2, axis = 1)\n",
    "\n",
    "    score =  numer / denom\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95d2731-a7ab-49c3-aca1-16e415a0fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_similarity_old(u: tf.Tensor, v: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "        Calculate pairwise similarity between two vectors\n",
    "            using cos distance.\n",
    "\n",
    "        args:\n",
    "            u: tf.Tensor - First input bector.\n",
    "            v: tf.Tensor - Second input vector.\n",
    "        returns:\n",
    "            score: tf.Tensor - Similarity score scalar.\n",
    "    \"\"\"\n",
    "\n",
    "    score = tf.tensordot(tf.transpose(u), v, axes = 1) / (tf.norm(u, 2) * tf.norm(v, 2))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "774b6443-7bb3-4f47-83dd-3324f603eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntxent_loss_new(batch: tf.Tensor,\n",
    "                temp: float = 1.0) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "        Normalised temperature-scaled cross entropy loss.\n",
    "\n",
    "        args:\n",
    "            batch: tf.Tensor - Batch of augmented tensors of size 2N.\n",
    "                Where N is the minibatch size.\n",
    "            temp: float - Temperate scale coefficient.\n",
    "        returns:\n",
    "            loss: tf.Tensor - Loss scalar.\n",
    "    \"\"\"\n",
    "\n",
    "    n_batch = tf.cast(tf.shape(batch)[0], tf.float32)\n",
    "    n_minibatch = tf.cast(n_batch / 2, np.int32)\n",
    "    loss = tf.constant(0.0)\n",
    "    \n",
    "    # get similarity matrix\n",
    "    sim_mat = pairwise_similarity(batch, batch)\n",
    "    sim_mat = tf.math.exp(sim_mat / temp)\n",
    "    \n",
    "    # calculate loss\n",
    "    for k in tf.range(n_minibatch, dtype = tf.int32):\n",
    "        loss_pairwise_1 = -1.0 * tf.math.log(sim_mat[k, (n_minibatch + k)] / (tf.reduce_sum(sim_mat[k,:k]) + tf.reduce_sum(sim_mat[k,(k + 1):])))\n",
    "        loss_pairwise_2 = -1.0 * tf.math.log(sim_mat[(n_minibatch + k), k] / (tf.reduce_sum(sim_mat[(n_minibatch + k),:(n_minibatch + k)]) + tf.reduce_sum(sim_mat[(n_minibatch + k),(n_minibatch + k + 1):])))\n",
    "        loss = loss + loss_pairwise_1 + loss_pairwise_2\n",
    "        \n",
    "    return loss / n_batch\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fc4ea2ab-82fb-4302-97aa-9d7e56b5283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntxent_loss_alt(batch_u: tf.Tensor,\n",
    "                    batch_v: tf.Tensor,\n",
    "                    temp: float = 1.0) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "        Normalised temperature-scaled cross entropy loss.\n",
    "\n",
    "        args:\n",
    "            batch: tf.Tensor - Batch of augmented tensors of size 2N.\n",
    "                Where N is the minibatch size.\n",
    "            temp: float - Temperate scale coefficient.\n",
    "        returns:\n",
    "            loss: tf.Tensor - Loss scalar.\n",
    "    \"\"\"\n",
    "\n",
    "    n_minibatch = batch_u.shape[0]\n",
    "    loss = tf.constant(0.0)\n",
    "    \n",
    "    # get similarity matrix\n",
    "    sim_mat = pairwise_similarity(batch_u, batch_v)\n",
    "    sim_mat = tf.math.exp(sim_mat / temp)\n",
    "    sim_mat_t = tf.transpose(sim_mat)\n",
    "    \n",
    "    # calculate loss\n",
    "    for k in tf.range(n_minibatch, dtype = tf.int32):\n",
    "        loss_pairwise_1 = -1.0 * tf.math.log(sim_mat[k, k] / tf.reduce_sum(sim_mat[k,:]))\n",
    "        loss_pairwise_2 = -1.0 * tf.math.log(sim_mat_t[k, k] / tf.reduce_sum(sim_mat_t[k,:]))\n",
    "        loss = loss + loss_pairwise_1 + loss_pairwise_2\n",
    "        \n",
    "    return loss / (2 * n_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1051268-732a-4fb5-9098-bd0bf569fba9",
   "metadata": {},
   "source": [
    "# Define Mock Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "899737c3-acf3-47c3-b862-e21d45663db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_1 = tf.random.normal((128,1024), dtype = tf.float32)\n",
    "test_input_2 = tf.random.normal((128,1024), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6f526b2-b2a2-4716-a97a-be0f1a3a6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_stacked = tf.concat([test_input_1,test_input_2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b675cde1-22dd-4671-9631-c4923fb2497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 218 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_1 = ntxent_loss(test_input_1, test_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "36241393-04ae-4eda-9057-6e1747e9206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 276 ms, sys: 7.4 ms, total: 283 ms\n",
      "Wall time: 278 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_2 = ntxent_loss_new(test_input_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f1c6aff-fa56-4a3c-8c20-2b56c49f4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 159 ms, sys: 3.65 ms, total: 162 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_3 = ntxent_loss_alt(test_input_1, test_input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62776db-c74a-4486-a966-54c57b0b2bde",
   "metadata": {},
   "source": [
    "## Precompute pairwise similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1dbd72-0f92-4ed1-b6cc-407634ebb61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.81 ms, sys: 3.53 ms, total: 13.3 ms\n",
      "Wall time: 4.65 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vectorised\n",
    "ps_vect = pairwise_similarity(test_input_stacked, test_input_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635ac3ab-4244-4845-b419-67cdf0cf5133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 104 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sequential\n",
    "for k_outer in tf.range(0, test_input_stacked.shape[0], dtype = tf.int32):\n",
    "    u = test_input_stacked[k_outer]\n",
    "    for k_inner in tf.range(0, test_input_stacked.shape[0], dtype = tf.int32):\n",
    "        v = test_input_stacked[k_inner]\n",
    "        score = pairwise_similarity_old(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdaa7db-78b4-4969-a86d-2f75aae8bcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
